{
   "cell_type": "markdown",
   "id": "48032f35",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
 }
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d156ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48032f35",
   "metadata": {},
   "source": [
    "### Processing Reddit Data in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c084cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Reddit Processing\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6d5e5",
   "metadata": {},
   "source": [
    "### Loading the Data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df481db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"True\").load(\"oldreddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bab00e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- upvote: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # Checking the Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcfa2b",
   "metadata": {},
   "source": [
    "As we can see, all the columns are string type using spark's automatic schema detection. Let's create a manual schema and assign new types to the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c57a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Manual Schema\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "StructField(\"id\", IntegerType(), True),\n",
    "StructField(\"title\", StringType(), True),\n",
    "StructField(\"subreddit\", StringType(), True),\n",
    "StructField(\"upvote\", LongType(), True),\n",
    "StructField(\"time\", IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4106f6",
   "metadata": {},
   "source": [
    "Now use our manual schema to load the data into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "622e525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "redditData = spark.read.format(\"csv\").schema(myManualSchema).option(\"header\", \"true\").load(\"oldreddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c78b30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- upvote: long (nullable = true)\n",
      " |-- time: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "redditData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e303697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+------+----+\n",
      "| id|               title|          subreddit|upvote|time|\n",
      "+---+--------------------+-------------------+------+----+\n",
      "|  1|NRG vs. G2 Esport...|  r/leagueoflegends|  8323|   8|\n",
      "|  2|\"What \"\"early int...|        r/AskReddit|  8324|   3|\n",
      "|  3|Temporary foreign...|           r/canada|  8325|   4|\n",
      "|  4|My wife served me...|   r/TrueOffMyChest|  1183|   4|\n",
      "|  5|To revive Canada’...|           r/canada|  1184|   3|\n",
      "|  6|I let my brother ...|r/mildlyinfuriating|  1185|   3|\n",
      "|  7|New York Times: H...|        r/worldnews|   451|   6|\n",
      "|  8|Teen admits to fi...|             r/news|   452|   6|\n",
      "|  9|Jack Dohertys sec...|   r/PublicFreakout|   453|   9|\n",
      "| 10|[Garrioch] We’ve ...|           r/hockey|  1977|   4|\n",
      "| 11|Our health system...|          r/ontario|  1978|   2|\n",
      "| 12|          Swiss Draw|  r/leagueoflegends|  1979|   6|\n",
      "| 13|AITA for disturbi...|    r/AmItheAsshole|   760|   5|\n",
      "| 14|Guy punched by se...|      r/AbruptChaos|   761|   6|\n",
      "| 15|It’s terrifying h...|  r/britishcolumbia|   762|   2|\n",
      "| 16|Scottie’s reactio...|   r/torontoraptors|  5198|   4|\n",
      "| 17|T1 vs. Bilibili G...|  r/leagueoflegends|  5199|   7|\n",
      "| 18|New evidence conf...|   r/onguardforthee|  5200|   2|\n",
      "| 19|Speaker Mike John...|         r/politics|  7079|   3|\n",
      "| 20|Why is chicken so...|          r/ontario|  7080|   4|\n",
      "+---+--------------------+-------------------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's take a glance at our data\n",
    "redditData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575f54e",
   "metadata": {},
   "source": [
    "### Spark Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9b525",
   "metadata": {},
   "source": [
    "Now, let's process this dataframe to generate a new dataframe that contains a list of subreddits with highest total upvotes in last 10 hours. This dataframe will help us identify which popular subreddit has most interactions in a given time frame. We can further upload the new dataframe on AWS S3 server for further processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0124f",
   "metadata": {},
   "source": [
    "We can achieve this using Sql Query as well as using PySpark, we'll try using both ways to see if there's any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b9be0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view for our data to use Sql\n",
    "redditData.createOrReplaceTempView(\"redditData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24479161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sql Query\n",
    "sqlWay = spark.sql('''\n",
    "SELECT subreddit, SUM(upvote) AS totalUpvotes\n",
    "FROM redditData\n",
    "WHERE time <= 10\n",
    "GROUP BY subreddit\n",
    "ORDER BY totalUpvotes desc;\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70018dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [totalUpvotes#64L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(totalUpvotes#64L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=56]\n",
      "      +- HashAggregate(keys=[subreddit#29], functions=[sum(upvote#30L)])\n",
      "         +- Exchange hashpartitioning(subreddit#29, 200), ENSURE_REQUIREMENTS, [plan_id=53]\n",
      "            +- HashAggregate(keys=[subreddit#29], functions=[partial_sum(upvote#30L)])\n",
      "               +- Project [subreddit#29, upvote#30L]\n",
      "                  +- Filter (isnotnull(time#31) AND (time#31 <= 10))\n",
      "                     +- FileScan csv [subreddit#29,upvote#30L,time#31] Batched: false, DataFilters: [isnotnull(time#31), (time#31 <= 10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/D:/Study/Sem3/AMOD5410/Ass2_5410/ques2/redspider/redspider/oldre..., PartitionFilters: [], PushedFilters: [IsNotNull(time), LessThanOrEqual(time,10)], ReadSchema: struct<subreddit:string,upvote:bigint,time:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e921e0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|           subreddit|totalUpvotes|\n",
      "+--------------------+------------+\n",
      "|            r/canada|      578690|\n",
      "|         r/worldnews|      456973|\n",
      "|            r/hockey|      442261|\n",
      "|r/justneckbeardth...|      318000|\n",
      "|       r/MadeMeSmile|      263595|\n",
      "|            r/comics|      263297|\n",
      "|             r/AITAH|      250199|\n",
      "|            r/movies|      250059|\n",
      "|          r/politics|      231109|\n",
      "|           r/ukraine|      196000|\n",
      "|            r/loseit|      195000|\n",
      "|    r/PoliticalHumor|      180512|\n",
      "|   r/datingoverforty|      169096|\n",
      "|        r/FrankOcean|      166000|\n",
      "|            r/bleach|      166000|\n",
      "|             r/memes|      165849|\n",
      "|r/WhitePeopleTwitter|      165114|\n",
      "|           r/pokemon|      165000|\n",
      "|     r/OldSchoolCool|      159000|\n",
      "|    r/wholesomememes|      156921|\n",
      "+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e0381",
   "metadata": {},
   "source": [
    "We can see the explain plan and the result obtained using the Sql Query, not let's do the same thing using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d37a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "pythonWay = redditData.filter(redditData.time <= 10)\\\n",
    "            .groupBy(\"subreddit\")\\\n",
    "            .agg(func.sum(\"upvote\").alias(\"totalUpvotes\"))\\\n",
    "            .orderBy(\"totalUpvotes\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9c8ecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [totalUpvotes#90L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(totalUpvotes#90L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=139]\n",
      "      +- HashAggregate(keys=[subreddit#29], functions=[sum(upvote#30L)])\n",
      "         +- Exchange hashpartitioning(subreddit#29, 200), ENSURE_REQUIREMENTS, [plan_id=136]\n",
      "            +- HashAggregate(keys=[subreddit#29], functions=[partial_sum(upvote#30L)])\n",
      "               +- Project [subreddit#29, upvote#30L]\n",
      "                  +- Filter (isnotnull(time#31) AND (time#31 <= 10))\n",
      "                     +- FileScan csv [subreddit#29,upvote#30L,time#31] Batched: false, DataFilters: [isnotnull(time#31), (time#31 <= 10)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/D:/Study/Sem3/AMOD5410/Ass2_5410/ques2/redspider/redspider/oldre..., PartitionFilters: [], PushedFilters: [IsNotNull(time), LessThanOrEqual(time,10)], ReadSchema: struct<subreddit:string,upvote:bigint,time:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pythonWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a51c7d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|           subreddit|totalUpvotes|\n",
      "+--------------------+------------+\n",
      "|            r/canada|      578690|\n",
      "|         r/worldnews|      456973|\n",
      "|            r/hockey|      442261|\n",
      "|r/justneckbeardth...|      318000|\n",
      "|       r/MadeMeSmile|      263595|\n",
      "|            r/comics|      263297|\n",
      "|             r/AITAH|      250199|\n",
      "|            r/movies|      250059|\n",
      "|          r/politics|      231109|\n",
      "|           r/ukraine|      196000|\n",
      "|            r/loseit|      195000|\n",
      "|    r/PoliticalHumor|      180512|\n",
      "|   r/datingoverforty|      169096|\n",
      "|        r/FrankOcean|      166000|\n",
      "|            r/bleach|      166000|\n",
      "|             r/memes|      165849|\n",
      "|r/WhitePeopleTwitter|      165114|\n",
      "|           r/pokemon|      165000|\n",
      "|     r/OldSchoolCool|      159000|\n",
      "|    r/wholesomememes|      156921|\n",
      "+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pythonWay.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d0a53",
   "metadata": {},
   "source": [
    "As we can see, both the explain plan and result is same using Sql and python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59ba826",
   "metadata": {},
   "source": [
    "### AWS S3 Hosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90b38f",
   "metadata": {},
   "source": [
    "Our processing steps are completed, now let's load the new dataframe on AWS S3 General Bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98de77",
   "metadata": {},
   "source": [
    "We are going to upload csv version of our output to AWS S3 server, hence let's convert the pyspark dataframe to a csv file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28549da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythonWay.toPandas().to_csv('subreddit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d642286",
   "metadata": {},
   "source": [
    "We will use the boto3 library to upload the csv file to our aws s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aaac91a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key='',\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "s3.meta.client.upload_file('subreddit.csv', 'myredditbucket', 'redditData/subreddits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd8fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
